{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOhhBpIy8Mype8JUegHuf0i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jitender2622/Chat-With-PDF-Gemini/blob/main/chat_with_pdf_gemini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 1. INSTALLATION\n",
        "# ==========================================\n",
        "# Installs necessary libraries for RAG, Vector Search, and PDF processing.\n",
        "# Note: Specific versions are pinned to ensure compatibility.\n",
        "# ==========================================\n",
        "\n",
        "!pip install -q -U langchain==0.3.0 \\\n",
        "langchain-community==0.3.0 \\\n",
        "langchain-core==0.3.0 \\\n",
        "langchain-google-genai \\\n",
        "langchain-huggingface \\\n",
        "langchain-text-splitters \\\n",
        "faiss-cpu \\\n",
        "pypdf \\\n",
        "sentence-transformers\n",
        "\n",
        "print(\"‚úÖ Installation Complete.\")\n",
        "print(\"‚ö†Ô∏è NOW: Go to 'Runtime' -> 'Restart Session' (or Restart Runtime).\")\n",
        "print(\"‚ö†Ô∏è Do NOT run this cell again after restarting.\")"
      ],
      "metadata": {
        "id": "5e-P_ojUNB4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 2. SETUP API & PATHS\n",
        "# ==========================================\n",
        "# Handles secure API key input and verifies that the target PDF exists.\n",
        "# ==========================================\n",
        "\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# Define the path to the source PDF\n",
        "pdf_filename = \"/content/terms.pdf\"  # <--- MAKE SURE THIS MATCHES YOUR FILE IN FILES TAB\n",
        "\n",
        "# Verify file existence to prevent downstream errors\n",
        "if not os.path.exists(pdf_filename):\n",
        "    raise FileNotFoundError(f\"‚ùå STOP: Please upload '{pdf_filename}' to the Files tab on the left!\")\n",
        "\n",
        "# Securely input Google API Key if not already set\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass(\"Paste your Google Gemini API Key here: \")"
      ],
      "metadata": {
        "id": "BibNInAGu90a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 3. LIBRARY IMPORTS\n",
        "# ==========================================\n",
        "# Importing LangChain components for document loading, splitting,\n",
        "# vector storage, and model interaction.\n",
        "# ==========================================\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "print(\"\\nüöÄ Libraries imported successfully!\")"
      ],
      "metadata": {
        "id": "WJyhaYM5vSKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 4. LOAD & SPLIT PDF\n",
        "# ==========================================\n",
        "# Loads the raw PDF text and splits it into chunks (1000 chars).\n",
        "# Overlap ensures context is preserved between chunks.\n",
        "# ==========================================\n",
        "\n",
        "print(\"üìÑ Loading and splitting PDF...\")\n",
        "\n",
        "loader = PyPDFLoader(pdf_filename)\n",
        "pages = loader.load()\n",
        "\n",
        "# Initialize splitter with overlap to maintain semantic continuity\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(pages)\n",
        "\n",
        "print(f\"‚úÖ Split into {len(splits)} chunks.\")"
      ],
      "metadata": {
        "id": "kUoG-vFUvix0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 5. EMBEDDINGS (LOCAL)\n",
        "# ==========================================\n",
        "# Converts text chunks into vector embeddings using a local HuggingFace model.\n",
        "# These vectors are stored in a FAISS index for efficient similarity search.\n",
        "# ==========================================\n",
        "\n",
        "print(\"üß† Creating Local Embeddings (HuggingFace)...\")\n",
        "\n",
        "# Using 'all-MiniLM-L6-v2' (Local CPU model) to avoid API rate limits\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Create the vector store\n",
        "vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)\n",
        "\n",
        "# Configure the retriever to fetch the top 5 most relevant chunks\n",
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
        "\n",
        "print(\"‚úÖ Vector Database Ready.\")"
      ],
      "metadata": {
        "id": "eaoNS81xvmO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 6. BUILD RAG CHAIN\n",
        "# ==========================================\n",
        "# Initializes the Gemini LLM and constructs the retrieval chain.\n",
        "# This links the Vector Store (Context) with the LLM (Reasoning).\n",
        "# ==========================================\n",
        "\n",
        "print(\"üîó Building the RAG Chain...\")\n",
        "\n",
        "# Initialize Gemini 2.0 Flash with low temperature for factual consistency\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.3)\n",
        "\n",
        "# Define the system instructions\n",
        "system_prompt = (\n",
        "    \"You are an assistant for question-answering tasks. \"\n",
        "    \"Use the following pieces of retrieved context to answer \"\n",
        "    \"the question. If you don't know the answer, say that you \"\n",
        "    \"don't know. Use three sentences maximum and keep the \"\n",
        "    \"answer concise.\"\n",
        "    \"\\n\\n\"\n",
        "    \"{context}\"\n",
        ")\n",
        "\n",
        "# Create the prompt template\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Assemble the processing chain\n",
        "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
        "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
        "\n",
        "print(\"‚úÖ RAG Chain Assembled.\")"
      ],
      "metadata": {
        "id": "z4KXN-glvqgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 7. EXECUTION\n",
        "# ==========================================\n",
        "# Runs a test query against the RAG chain.\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nü§ñ SYSTEM READY! Testing with a question...\")\n",
        "\n",
        "# Define your query here\n",
        "query = \"What is Transformers?\"\n",
        "print(f\"‚ùì Question: {query}\")\n",
        "\n",
        "# Invoke the chain\n",
        "response = rag_chain.invoke({\"input\": query})\n",
        "\n",
        "# Output the result\n",
        "print(\"\\nüí° Answer:\")\n",
        "print(response[\"answer\"])"
      ],
      "metadata": {
        "id": "_qz7XjSvv33J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wP-t6M4sv68Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}